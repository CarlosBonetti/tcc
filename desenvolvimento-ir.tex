% !TEX root = main.tex

\chapter{Uso de \textit{Feedback} de relevância} \label{cap:avaliacao}

Um dos itens de melhoria indicados por \cite{Silq1} após a realização da primeira versão do sistema seria ``permitir que o usuário auxilie a ferramenta na qualificação", conforme relatado na seção \ref{sec:trabalhos-futuros-silq1}. Em sistemas de IR, esta característica é denominada \textit{Feedback de Relevância}.

A ideia por trás desta técnica é permitir que o usuário julgue resultados iniciais retornados pelo sistema, classificando-os como relevantes ou não para a \textit{query} vigente. O sistema então é capaz de utilizar esta informação para melhorar seu algoritmo de classificação e retornar melhores resultados para novas \textit{queries}.

O capítulo atual apresenta como esta técnica foi aplicada no SILQ, qual modelo lógico foi utilizado para o armazenamento dos \textit{feedbacks} e qual técnica utilizada para sua obtenção. Por fim, são apresentadas medições realizadas no sistema que validam se a técnica foi relevante para o aumento de performance.

\section{Obtenção do feedback} \label{sec:feedback}

Em sistemas de IR que implementam \textit{feedback} de relevância, tipicamente é permitido ao usuário julgar como relevante ou não relevante cada item retornado pelo sistema. Isso é natural pois o objetivo deste tipo de sistema é retornar a totalidade do conjunto de itens relevantes, cujo tamanho é variável dependendo da \textit{query}. O SILQ, porém, é um caso específico em que existem somente 0 ou 1 item relevante para toda \textit{query}. Isso acontece pois cada trabalho indicado no currículo Lattes de um pesquisador é marcado como publicado ou apresentado em somente um evento. O objetivo do algoritmo de avaliação do SILQ é deduzir que evento é este, caso exista na base de dados Qualis.

Para a implementação de \textit{feedback} de relevância no SILQ, portanto, não é necessário o julgamento de cada item retornado pelo sistema em uma avaliação, mas apenas marcar \textit{qual} dos itens da base de dados Qualis é um \textit{match} correto para a \textit{query} vigente. Desta forma, o usuário deve ser capaz de indicar o registro Qualis que deve ser considerado para cada trabalho avaliado. Também existe o caso especial em que não existe um registro Qualis correspondente ao trabalho, neste caso o usuário deve ser capaz de indicar que não existem \textit{matches} corretos para o trabalho.

A Figura \ref{fig:feedbacks} mostra o exemplo de três trabalhos conceituados, em uma avaliação de currículo Lattes, e que receberam \textit{feedback} do usuário. O botão de ``joinha" é utilizado para marcar o resultado que o usuário considera relevante para cada trabalho (os botões grifados marcam aqueles que já foram marcados). Neste caso, o registro Qualis marcado como relevante é associado à \textit{query} atual, que engloba o título, ano e área do trabalho avaliado, juntamente com o usuário que está realizando o julgamento. Também é possível marcar a opção ``Nenhum registro Qualis correspondente" ou sugerir algum resultado não retornado previamente pelo sistema em ``Sugerir matching".

\begin{figure}[!h]
   \centering
   \caption{Detalhe dos controles de feedback de relevância na página de avaliação de currículo Lattes}
   \includegraphics[width=\textwidth]{figuras/feedbacks.png}
   \label{fig:feedbacks}
\end{figure}

Os \textit{feedbacks} de relevância dados pelos usuários são salvos na base de dados SILQ na tabela \textit{tb\_feedback} (Figura \ref{fig:modelo-logico}) para uso posterior pelo algoritmo de classificação, conforme descrito na seção \ref{sec:algoritmo}. Já que um trabalho qualquer pode ter no máximo um registro Qualis associado, são salvas somente um \textit{feedback} por \textit{query} por usuário. Desta forma, uma chave única é utilizada na tabela de \textit{feedbacks}, dada pela dupla (título do trabalho, usuário).

\section{Algoritmos de avaliação com feedback} \label{sec:algoritmo}

Uma vez registrados os \textit{feedbacks} dos usuários, passa-se a questionar de que forma utilizá-los para aumentar a performance do sistema. A seção atual apresenta dois algoritmos que foram testados neste trabalho e como foram implementados, enquanto a próxima seção apresenta os resultados e comparações de precisão dos mesmos.

\subsection{Algoritmo \textit{fb(t)}}

Umas das abordagens mais simples que podem ser usadas neste caso é utilizar o resultado marcado pelo usuário sempre que uma \textit{query} idêntica ao do \textit{feedback} seja submetida ao sistema. O algoritmo \textit{fb(1.0)}, portanto, foi desenvolvido com base nesta ideia. O valor ``$1.0$" presente no nome do algoritmo apenas indica que o \textit{feedback} é considerado em detrimento de qualquer outro resultado dado pelo sistema quando a \textit{query} submetida for 100\% similar (ou seja, idêntica) à \textit{query} do \textit{feedback}.

\begin{lstlisting}[language=java, caption=Algoritmo \textit{fb(1.0)}]
public List<Conceito> avaliar(String query, Usuario usuarioLogado) {
   List<Conceito> conceitos = getConceitosTrigrams(query);

   Feedback feedback = getConceitoFeedback(query, usuarioLogado);
   Conceito feedbackConceito = feedbackToConceito(feedback);
   conceitos.add(feedbackConceito);

   return conceitos;
}
\end{lstlisting}

Uma modificação que pode ser feita no mesmo algoritmo é utilizar \textit{thresholds de feedback} mais baixos. O \textit{fb(1.0)} considera apenas \textit{feedbacks} que sejam idênticos à \textit{query} submetida, porém pode ser que sejam obtidos resultados melhores ao considerar \textit{feedbacks} textualmente similares à \textit{query} atual, considerando certo \textit{threhold}. O algoritmo \textit{fb(t)} (para $0.0 \le t \le 1.0$) é uma generalização de \textit{fb(1.0)} que considera \textit{feedbacks} cuja similaridade textual em relação à \textit{query} seja maior que o \textit{treshold} $t$. Por exemplo, \textit{fb(0.75)} irá considerar \textit{feedbacks} cujo valor de similaridade textual em relação à \textit{query} seja $0.75$ ou superior. Neste caso o \textit{feedback} é retornado em detrimento dos demais resultado encontrados pelo sistema.

O algoritmo \textit{fb(t)}, entretanto, leva a outros questionamentos, já que utiliza a mesma técnica de \textit{data matching} que foi proposta a melhorar. Qual o valor de $t$ (\textit{trehshold de feedback}) ideal? Qual o algoritmo de similaridade textual ideal para este caso? A seção \ref{sec:avaliacao-algoritmos} apresenta testes de validação do algoritmo \textit{fb(t)} para diferentes valores de $t$.

\subsection{Algoritmo \textit{query\_aliasing}}

Outra estratégia que mostrou-se de mais fácil implementação e melhor desempenho foi considerar os \textit{feedbacks} do usuário e inserí-los no \textit{rank} de resultados do sistema com base em seu valor de similaridade textual em relação à \textit{query} submetida.

Ou seja, ao processar uma \textit{query} $q$ qualquer, o sistema processa um \textit{rank} de resultados primários com base no algoritmo \textit{trigrams} inicial. O \textit{rank} é ordenado do resultado mais similar à $q$ ao menos similar. Após esta etapa, ele também compara $q$ com cada uma das \textit{queries} atreladas aos \textit{feedbacks} de relevância dados pelo usuário utilizando o mesmo algoritmo de similaridade textual (\textit{trigrams} neste caso). O resultado mais similar, digamos $f$, é inserido no \textit{ranking} de resultados.

Desta forma, se $q$ é idêntico a um \textit{feedback} já submetido pelo usuário, $f$ será retornado e inserido no topo do \textit{ranking} de resultados, por ser 100\% similar à \textit{query}. Outros resultados similares, porém não idênticos, serão inseridos no \textit{ranking} conforme seu valor de similaridade e só serão escolhidos em detrimento de outros resultados primários se seus valores de similaridade forem superiores a eles.

É como se, ao dar um \textit{feedback} de relevância qualquer, o usuário criasse um \textit{alias} (um apelido) ao resultado que está sugerindo. Assim, o sistema deve avaliar novas \textit{queries} não só comparando-as com o nome real dos documentos, mas também com os apelidos dados a eles pelo usuário. Por este motivo o algoritmo foi chamado de \textit{query\_aliasing}. A avaliação deste algoritmo foi realizada e comparada com os demais na seção \ref{sec:avaliacao-algoritmos}.

\section{Avaliação experimental} \label{sec:validation}

Esta seção apresenta os procedimentos realizados para avaliar as alterações promovidas no algoritmo de avaliação da nova versão do SILQ e se elas contribuíram para o aumento da precisão do sistema.

\subsection{Conjunto de testes}

O conjunto de testes utilizado para a avaliação do sistema foi criado a partir dos currículos Lattes de 33 pesquisadores do programa de pós-graduação em Ciência da Computação da Universidade Federal de Santa Catarina (UFSC).

Destes 33 currículos, 300 publicações foram selecionadas de forma aleatória e manualmente avaliadas: caso possuíssem um registro Qualis equivalente então a publicação juntamente com o Qualis associado eram salvos no conjunto de testes; caso não possuíssem registro Qualis equivalente, então eram marcados como tal e também adicionados ao conjunto de testes.

Nas avaliações descritas a seguir, foram dadas como \textit{query} ao sistema cada uma das publicações da coleção de testes, porém sem expor os resultados manualmente avaliados. Cada resposta retornada pelo sistema foi comparada com a respectiva resposta manualmente selecionada. Em caso das respostas serem idênticas, então o sistema avaliou corretamente a publicação; em caso de não serem idênticas, avaliou incorretamente. O caso de não haver registro Qualis equivalente à publicação foi considerada uma resposta correta quando o sistema não retornou nenhum resultado, e uma resposta incorreta caso contrário.

\subsection{Avaliação de \textit{threshold} ideal}

 Um dos questionamentos levantados no início deste trabalho e que geralmente ocorre ao projetar sistemas de \textit{data matching} baseados em similaridade, é o de qual \textit{threshold} utilizar. Na primeira versão do sistema foi introduzido um controle de ``nível de confiança" que permitia ao usuário controlar o \textit{threshold} utilizado pelo algoritmo, conforme detalhado na seção \ref{sec:algoritmo-silq1}. O nível de confiança padrão, porém, foi fixado em 60\% (equivalente ao \textit{threshold} de valor $0.6$). Este valor foi provavelmente escolhido de forma empírica pois observou-se que maximizava o número de resultados corretos, porém não foram realizados experimentos comprovando esta teoria.

 Desta forma, para encontrar o valor de \textit{threshold} ideal utilizamos nosso conjunto de testes para avaliar o algoritmo inicial do SILQ 1, em um primeiro momento. O método utilizado foi o de avaliar via sistema cada uma das publicações do conjunto de testes e comparar com o resultado real, e repetir o processo variando o \textit{threshold} a fim de observar as médias de exatidão e de rank recíproco (MRR). Os resultados foram agrupados no gráfico da Figura \ref{fig:avaliacao-threshold}. A linha em azul claro representa a exatidão, ou seja, a taxa de trabalhos corretamente avaliados pelo sistema. A linha em cinza representa a média de rank recíproco (MRR), calculada conforme discutido na seção \ref{sec:mrr}.

 \begin{figure}[!h]
    \centering
    \caption{Taxa de trabalhos corretamente avaliados e Média de Rank Recíproco (MRR) para diferentes \textit{thresholds}}
    \includegraphics[width=\textwidth]{figuras/avaliacao-threshold.png}
    \label{fig:avaliacao-threshold}
 \end{figure}

O primeiro fenômeno que observamos ao avaliar o gráfico é o ponto de máximo por volta do valor $0.55$ de \textit{threshold}, que totaliza uma exatidão aproximada de 88\%, e a tendência da exatidão baixar ao se afastar deste pico, para ambas as direções. Esse é um comportamento esperado pois valores de \textit{threhold} baixos tendem a diminuir a precisão do sistema por retornar resultados não relevantes para as \textit{queries}, enquanto valores altos tendem a dimuir a precisão por deixar de retornar resultados relevantes. Este ponto máximo trata-se, portanto, do \textit{threshold} ideal para o caso de testes em questão.

Outra característica observada é a tendência do valor de MRR acompanhar o da exatidão, sendo sempre igual ou apenas um pouco superior em magnetude. Isso acontece pela forma com que o MRR é calculado, atribuindo valor de $1/r$ a cada avaliação, sendo $r$ a posição em que o resultado real foi avaliado pelo sistema. Se o resultado foi corretamente avaliado, portanto, o valor de $1/1 = 1$ é atribuído ao resultado, o mesmo valor que seria atribuído à exatidão (cuja imagem é somente $\{0, 1\}$ para cada resultado). A semelhança dos valores, portanto, indica que houveram poucos casos em que o algoritmo retornou o resultado real em posições inferiores à primeira no \textit{rank} de avaliação. Esta característica do valor de MRR permaneceu constante nos demais testes realizados neste trabalho, portanto omitiu-se o valor de MRR nas demais avaliações.

\subsection{Avaliação dos algoritmos} \label{sec:avaliacao-algoritmos}

Os algoritmos descritos na seção \ref{sec:algoritmo} foram avaliados utilizando o mesmo processo descrito na seção anterior. O algoritmo \textit{trgm} trata-se do algoritmo \textit{trigrams} inicial utilizado pelo SILQ 1 e cuja análise de \textit{threhold} ideal foi realizada na seção anterior. O algoritmo \textit{fb(t)} foi testado variando $t$ nos valores que obtiveram melhores resultados. Todas as análise foram realizadas com valor de \textit{treshold} igual a $0.55$. A Tabela \ref{tab:comparacao-algoritmos} apresenta os resultados de cada teste.

\begin{table}[!h]
\begin{center}
\caption{Comparação da exatidão dos diferentes algoritmos testados (utilizando \textit{threshold} de $0.55$)}
\label{tab:comparacao-algoritmos}
\begin{tabular}{ c | c }
\hline
\textbf{Algoritmo} & \textbf{Exatidão} \\
\hline

\textit{trgm} & 88.667\% \\
\textit{trgm + fb(1.00)} & 89.667\% \\
\textit{trgm + fb(0.90)} & 90.667\% \\
\textit{trgm + fb(0.80)} & 92.667\% \\
\textit{trgm + fb(0.70)} & 92.667\% \\
\textit{trgm + fb(0.60)} & 91.000\% \\
\textit{trgm + query\_aliasing} & \textbf{93.333}\% \\

\end{tabular}
\end{center}
\end{table}

A primeira tentativa de usar \textit{feedback} de relevância na avaliação foi com o algoritmo \textit{fb(1.00)}, que considera os resultados informados pelo usuário quando a \textit{query} é idêntica a algum \textit{feedback}. Houve uma melhora na taxa de acertos, porém de forma não tão significativa.

As variações que utilizam valores menores de $t$, porém, obtiveram melhores resultados, por serem capazes de identificar \textit{queries} similares aos \textit{feedbacks} já informados pelo usuário, mesmo quando este não julgou exatamente a \textit{query} em questão. Um exemplo que demonstra este fato são os nomes de eventos \textit{``IEEE International Symposium on Computer-Based Medical Systems"} e \textit{``27th International Symposium on ComputerBased Medical Systems (CBMS)"}, extraídos de um mesmo currículo Lattes. É fácil notar que tratam-se do mesmo evento, porém o usuário informou a sigla e o número da edição no segundo, e nenhuma destas informações no primeiro (além de não utilizar o hífen em um dos casos). Caso o usuário tenha dado \textit{feedback} para somente um dos casos, os algoritmos \textit{fb(t)} com valores de $t$ inferiores a $1.0$ são capazes de utilizar o mesmo \textit{feedback} para ambos, apesar das \textit{queries} não serem idênticas.

Valores de $t$ muito baixos, porém, deterioram rapidamente a taxa de acerto pois consideram \textit{feedbacks} similares entre eventos que não tem relação. Desta forma, existe também um ``valor ideal" de $t$, que gira em torno de $0.7$ a $0.8$ conforme os testes realizados.

O algoritmo \textit{fb(t)}, porém, pode cometer ``injustiças" pois considera os \textit{feedbacks} como resultados corretos, independente dos resultados primários retornados, caso sejam superiores ao \textit{threshold} $t$. O algoritmo de \textit{query\_aliasing} resolve este problema inserindo o \textit{feedback} no \textit{ranking} junto com os demais resultados. O melhor resultado (aquele mais similar à \textit{query}) será utilizado, independente da técnica usada para obtê-lo. Nos testes realizados, o algoritmo de \textit{query\_aliasing} obteve a melhor taxa de acerto, com uma média de 93.3\% de trabalhos corretamente avaliados.

Foi realizada uma última avaliação comparativa entre o algoritmo \textit{trgm} inicial e o novo que utiliza \textit{query\_aliasing}. A Figura \ref{fig:avaliacao-algoritmos} mostra a taxa de acerto média para ambos os algoritmos variando o \textit{threshold} utilizado. Nota-se que o algoritmo que utiliza \textit{feedback} de relevância obteve melhores resultados, para qualquer \textit{threshold} utilizado, aumentando em aproximadamente 6\% a taxa de acerto. O \textit{threshold} ideal, porém, se manteve constante em $0.55$.

\begin{figure}[!h]
   \centering
   \caption{Comparação da taxa de acerto do algoritmo \textit{trgm} e do \textit{trgm + query\_aliasing} para diferentes \textit{thresholds}}
   \includegraphics[width=\textwidth]{figuras/avaliacao-algoritmos.png}
   \label{fig:avaliacao-algoritmos}
\end{figure}
