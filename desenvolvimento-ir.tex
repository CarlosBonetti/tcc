% !TEX root = main.tex

\chapter{Uso de \textit{Feedback} de relevância} \label{cap:avaliacao}

Como já mencionado, um dos itens de melhoria indicados por \cite{Silq1} após a realização da primeira versão do sistema seria \quotes{permitir que o usuário auxilie a ferramenta na qualificação}. Em sistemas de IR, esta característica é denominada \textit{Feedback de Relevância}.

A ideia por trás desta técnica é permitir que o usuário julgue resultados iniciais retornados pelo sistema, classificando-os como relevantes ou não para a \textit{query} vigente. O sistema então é capaz de utilizar esta informação para melhorar seu algoritmo de classificação e retornar melhores resultados para novas \textit{queries}.

O capítulo atual apresenta como esta técnica foi aplicada no SILQ, qual modelo lógico foi utilizado para o armazenamento dos \textit{feedbacks} e qual técnica utilizada para sua obtenção. Por fim, são apresentadas medições realizadas no sistema que validam se a técnica foi relevante para o aumento de performance.

\section{Obtenção de feedback} \label{sec:feedback}

Em sistemas de IR que implementam \textit{feedback} de relevância, tipicamente é permitido ao usuário julgar como relevante ou não relevante cada item retornado pelo sistema. Isso é natural pois o objetivo deste tipo de sistema é retornar a totalidade do conjunto de itens relevantes, cujo tamanho é variável dependendo da \textit{query}. O SILQ, porém, é um caso específico em que existem somente 0 ou 1 item relevante para toda \textit{query}. Isso acontece pois cada trabalho indicado no currículo Lattes de um pesquisador aparece qualificado no Qualis apenas uma única vez no ano em que o pesquisador o publicou. O objetivo do algoritmo de avaliação do SILQ é deduzir que registro Qualis é esse, caso exista na base de dados.

Para a implementação de \textit{feedback} de relevância no SILQ, portanto, não é necessário o julgamento de cada item retornado pelo sistema em uma avaliação, mas apenas marcar \textit{qual} dos itens da base de dados Qualis é um \textit{match} correto para a \textit{query} vigente. Desta forma, o usuário deve ser capaz de indicar o registro Qualis que deve ser considerado para cada trabalho avaliado. Também existe o caso especial em que não existe um registro Qualis correspondente ao trabalho, neste caso o usuário deve ser capaz de indicar que não existem \textit{matches} corretos para o trabalho.

A Figura \ref{fig:feedbacks} mostra o exemplo de três trabalhos extraídos de um currículo Lattes e avaliados pelo SILQ que receberam \textit{feedback} do usuário. O botão de \quotes{joinha} é utilizado para marcar o resultado que o usuário considera relevante para cada trabalho. Os botões grifados (com fundo azul) representam que o resultado já foi previamente marcado. Neste caso, o registro Qualis marcado como relevante é associado à \textit{query} atual, que engloba o título, ano e área do trabalho avaliado, juntamente com o usuário que está realizando o julgamento. Também é possível marcar a opção \quotes{Nenhum registro Qualis correspondente} ou sugerir algum resultado não retornado previamente pelo sistema em \quotes{Sugerir matching} (mostrado no canto inferior direito da Figura exemplo).

\begin{figure}[!h]
   \centering
   \caption{Detalhe dos controles de feedback de relevância na página de avaliação de currículo Lattes}
   \includegraphics[width=\textwidth]{figuras/feedbacks.png}
   \label{fig:feedbacks}
\end{figure}

Os \textit{feedbacks} de relevância dados pelos usuários são salvos na base de dados SILQ na tabela \textit{tb\_feedback} (Figura \ref{fig:modelo-logico}) para uso posterior pelo algoritmo de classificação, conforme descrito na Seção \ref{sec:algoritmo}. Já que um trabalho qualquer pode ter no máximo um registro Qualis associado, é salvo somente um \textit{feedback} por \textit{query} por usuário. Desta forma, uma chave única é utilizada na tabela de \textit{feedbacks}, dada pela dupla (título do trabalho, usuário).

\section{Algoritmos de avaliação com feedback} \label{sec:algoritmo}

Uma vez registrados os \textit{feedbacks} dos usuários, passa-se a questionar de que forma utilizá-los para aumentar a taxa de acerto do sistema para futuras consultas. A Seção atual apresenta dois algoritmos que foram testados neste trabalho e como foram implementados, enquanto a próxima Seção apresenta os resultados e comparações de exatidão dos mesmos.

\subsection{Algoritmo \texttt{fb(t)}}

Umas das abordagens mais simples que podem ser usadas neste caso é utilizar o resultado marcado pelo usuário sempre que uma \textit{query} idêntica ao do \textit{feedback} seja submetida ao sistema. O algoritmo \texttt{fb(1.0)}, portanto, foi desenvolvido com base nesta ideia. O valor \quotes{$1.0$} presente no nome do algoritmo apenas indica que o \textit{feedback} é considerado em detrimento de qualquer outro resultado dado pelo sistema quando a \textit{query} submetida for 100\% similar (ou seja, idêntica) à \textit{query} do \textit{feedback}.

Pode-se dar o exemplo real de um nome de evento extraído de um currículo Lattes cadastrado pelo pesquisador como \quotes{Software Engineering Knowledge Engineering}, no ano de 2009 e com área de avaliação Ciência da Computação. Ao avaliar tal trabalho, o sistema retorna a lista da Tabela \ref{tab:exemplo-fb1}.

\begin{table}[!h]
\begin{center}
\caption{Resultados retornados pelo SILQ para a \textit{query} \textit{\quotes{Software Engineering Knowledge Engineering}}} \label{tab:exemplo-fb1}
\begin{tabular}{ c | p{7cm} | c }
\hline
\textbf{\#} & \textbf{Evento} & \textbf{Similaridade} \\ \hline
1 & Software Engineering and Data Engineering (SEDE) & 0.53 \\ \hline
2 & International Conference on Software Engineering and Knowledge Engineering (SEKE) & 0.49 \\ \hline
3 & Software Engineering and Applications (SEA\_A) & 0.45 \\ \hline
4 & Software Engineering for Secure Systems (SESS) & 0.45 \\ \hline
5 & Annual Software Engineering Workshop (SEW) & 0.45 \\
\hline
\end{tabular}
\end{center}
\end{table}

Após analisar esta lista, o usuário submeteu um \textit{feedback} ao sistema marcando o resultado \#2 como o correto. Desta forma, utilizando o algoritmo \texttt{fb(1.0)}, toda \textit{query} subsequente idêntica a \quotes{Software Engineering Knowledge Engineering} terá o resultado \#2 retornado na primeira posição.

O \texttt{fb(1.0)} considera apenas \textit{feedbacks} que sejam idênticos à \textit{query} submetida, \textit{queries} similares não são consideradas. O usuário do exemplo anterior possui um outro trabalho cadastrado em seu currículo Lattes cujo título de evento é \quotes{Software Engineering \textbf{and} Knowledge Engineering}. Pode-se deduzir que o usuário quis se referir ao mesmo evento, porém o título não é idêntico ao exemplo anterior por causa do termo \textit{\quotes{and}}. Neste caso, o algoritmo \texttt{fb(1.0)} não é capaz de deduzir que os dois casos se referem ao mesmo evento, apesar da semelhança entre eles. Uma modificação que pode ser realizada no algoritmo é utilizar uma função de similaridade entre novas \textit{queries} submetidas ao sistema com aquelas anteriormente submetidas e que possuem \textit{feedback} do usuário. Se a similaridade entre a nova \textit{query} e algum dos \textit{feedbacks} for maior do que certo \textit{threshold de feedback}, então é provável que a nova \textit{query} se refira ao mesmo evento do \textit{feedback} anteriormente fornecido.

O algoritmo \texttt{fb(t)} (para $0.0 \le t \le 1.0$) é uma generalização de \texttt{fb(1.0)} que considera \textit{feedbacks} cuja similaridade textual em relação à \textit{query} seja maior que o \textit{treshold} $t$. Por exemplo, \textit{fb(0.75)} irá considerar \textit{feedbacks} cujo valor de similaridade textual em relação à \textit{query} seja $0.75$ ou superior. No exemplo anterior, ao submeter a nova \textit{query} \quotes{Software Engineering and Knowledge Engineering} ao sistema, o algoritmo \textit{fb(0.75)} calcula a similaridade entre ela e os \textit{feedbacks} anteriores fornecidos pelo usuário e encontra o \textit{feedback} da primeira \textit{query} \quotes{Software Engineering Knowledge Engineering} por ser 88\% similar à \textit{query} atual. Neste caso, por ter uma similaridade maior do que o \textit{threshold} de $0.75$ estipulado, o algoritmo retorna o mesmo evento marcado no \textit{feedback} para a \textit{query} atual (o evento \#2 da Tabela \ref{tab:exemplo-fb1}).

O algoritmo \texttt{fb(t)}, entretanto, leva a outros questionamentos, já que utiliza a mesma técnica de \textit{data-matching} que foi proposta a melhorar. Qual o valor de $t$ (\textit{treshold de feedback}) ideal? Qual o algoritmo de similaridade textual ideal para este caso? A Seção \ref{sec:avaliacao-algoritmos} apresenta testes de validação do algoritmo \texttt{fb(t)} para diferentes valores de $t$.

O Algoritmo \ref{alg:fbt} é a representação em pseudocódigo de \texttt{fb(t)}. O parâmetro $q$ representa a \textit{query}, $t$ é o valor de \textit{threshold de feedback}, $D$ é o conjunto de todos os documentos a serem pesquisados e $F$ o conjunto de \textit{feedbacks} fornecidos contendo as duplas ($q_f, d$), \textit{query} do \textit{feedback} e documento dado como \textit{feedback}, respectivamente, tal que $d \in D$. A saída $m$ é o registro com maior probabilidade de ser um \textit{match} correto para a \textit{query}. A variável $R$ é uma lista 0-indexada contendo os resultados da consulta, ordenada por ordem decrescente da probabilidade do resultado ser um \textit{match} correto para a \textit{query}. A função \texttt{trigram\_sim} calcula a similaridade textual entre duas \textit{strings} utilizando o método \textit{trigrams} e retornando um valor no intervalo $[0, 1]$. A função \texttt{trigram\_rank} é o algoritmo de avaliação da primeira versão do SILQ, que cria o rank $R$ de similaridade a partir da comparação entre $q$ e cada um dos documentos de $D$.

\begin{algorithm}
\caption{\texttt{fb(t)}}
\label{alg:fbt}
\LinesNumbered
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{$q, t, D, F$}
\Output{$m$}

$R \gets \texttt{trigram\_rank(q, D)}$

$m \gets R[0]$

$s_m \gets -1$

\For{$ (q_f, d) \in F$}{
   $s \gets \texttt{trigram\_sim($q, q_f$)}$

	\If{$s \ge t$ \texttt{and} $s \ge s_m$}{
		$m \gets d$

      $s_m \gets s$
	}
}
\KwRet{$m$} \label{li:sum-zero-begin}
\end{algorithm}

\subsection{Algoritmo \texttt{query\_aliasing}}

Uma adaptação de \texttt{fb(t)} que mostrou-se de mais fácil implementação e que não gera o questionamento de qual valor de $t$ utilizar, foi considerar as \textit{queries} de \textit{feedbacks} anteriormente fornecidos pelo usuário, da mesma forma que o \texttt{fb(t)}, porém inseri-las no \textit{rank} de resultados de novas \textit{queries} submetidas com base em seus valores de similaridade textual em relação à nova \textit{query}, junto com os resultados previamente selecionados. Assim, ao invés de escolhê-lo em detrimento dos demais, o evento marcado com \textit{feedback} só é retornado se for mais bem ranqueado que os demais resultados.

Considerando os mesmos exemplos dados na Seção anterior, em que o usuário submete a nova \textit{query}  \quotes{Software Engineering and Knowledge Engineering} ao sistema, o algoritmo de \texttt{query\_aliasing} realiza comparação textual entre a nova \textit{query} e as \textit{queries} anteriores que possuam \textit{feedback}, da mesma forma que o \texttt{fb(t)}, encontrando a \textit{query} \quotes{Software Engineering Knowledge Engineering}, com um valor de similaridade de $0.88$. Ao contrário do $fb(t)$, o algoritmo de \texttt{query\_aliasing} irá inserir o evento dado como \textit{feedback} a esta \textit{query} junto com a lista de resultados previamente encontrados apenas via similaridade textual, usando o valor de $0.88$ para posicionamento no \textit{ranking}. A Tabela \ref{tab:exemplo-qa} mostra o \textit{ranking} retornado para este exemplo. Nota-se que o evento \#2, marcado pelo usuário como correto, foi elevado no \textit{ranking} por receber o novo valor de similaridade da comparação com o \textit{feedback}.

\begin{table}[!h]
\begin{center}
\caption{Resultados retornados pelo SILQ para a \textit{query} \textit{\quotes{Software Engineering and Knowledge Engineering}} utilizando \texttt{query\_aliasing}} \label{tab:exemplo-qa}
\begin{tabular}{ c | p{7cm} | c }
\hline
\textbf{\#} & \textbf{Evento} & \textbf{Similaridade} \\ \hline
2 & International Conference on Software Engineering and Knowledge Engineering (SEKE) & 0.88 \\ \hline
1 & Software Engineering and Data Engineering (SEDE) & 0.62 \\ \hline
3 & Software Engineering and Applications (SEA\_A) & 0.53 \\ \hline
... & ... & ... \\ \hline
\end{tabular}
\end{center}
\end{table}

O valor de similaridade atribuído, porém, perde seu significado semântico pois não é mais a similaridade textual entre o título do evento do Lattes e do Qualis calculado através do \textit{trigrams}, mas um valor adimensional usado apenas para ordenação relativa dentro do \textit{ranking}.

Desta forma, ao processar uma \textit{query} $q$ qualquer, o sistema processa um \textit{rank} de resultados primários com base no algoritmo \textit{trigrams} inicial. O \textit{rank} é ordenado do resultado mais similar à $q$ ao menos similar. Após esta etapa, ele também compara $q$ com cada uma das \textit{queries} anteriormente submetidas pelo usuário e que possuem \textit{feedback} de relevância utilizando o mesmo algoritmo de similaridade textual. O resultado mais similar é inserido no \textit{ranking} de resultados. Assim, se $q$ é idêntico a um \textit{feedback} já submetido pelo usuário, o evento deste \textit{feedback} será retornado e inserido no topo do \textit{ranking} de resultados, por ser 100\% similar à \textit{query}. Outros resultados similares, porém não idênticos, serão inseridos no \textit{ranking} conforme seu valor de similaridade e só serão escolhidos em detrimento de outros resultados primários se seus valores de similaridade forem superiores a eles.

É como se, ao dar um \textit{feedback} de relevância qualquer, o usuário criasse um \textit{alias} (um apelido) ao resultado que está sugerindo. Assim, o sistema deve avaliar novas \textit{queries} não só comparando-as com o nome real dos documentos, mas também com os apelidos dados a eles pelo usuário. Por este motivo o algoritmo foi chamado de \texttt{query\_aliasing}. A avaliação deste algoritmo foi realizada e comparada com os demais na Seção \ref{sec:avaliacao-algoritmos}.

O Algoritmo \ref{alg:qa} é a representação em pseudocódigo do método proposto. O significado semântico das variáveis é equivalente ao do Algoritmo \ref{alg:fbt}. A saída deste algoritmo, porém, é o próprio \textit{rank} $R$, possivelmente contendo novos resultados devido à comparação com os \textit{feedbacks} de $F$. O método \texttt{insert\_ordered} da linha \ref{lin:insert} insere o registro $d$ na lista ordenada $R$ com um valor de \textit{rank} $s$, preservando a ordenação da lista, de forma que o elemento em $R[0]$ seja aquele com maior valor de \textit{rank} e, assim, o registro com maior probabilidade de ser um \textit{match} correto para a \textit{query}.

\begin{algorithm}
\caption{\texttt{query\_aliasing}}
\label{alg:qa}
\LinesNumbered
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{$q, D, F$}
\Output{$R$}

$R \gets \texttt{trigram\_rank(q, D)}$

\For{$ (q_f, d) \in F$}{
   $s \gets \texttt{trigram\_sim($q, q_f$)}$

   $\texttt{insert\_ordered(R, s, d)}$ \label{lin:insert}
}
\KwRet{$R$} \label{li:sum-zero-begin}
\end{algorithm}

\section{Avaliação experimental} \label{sec:validation}

Esta Seção apresenta os procedimentos realizados para avaliar as alterações promovidas no algoritmo de avaliação da nova versão do SILQ e se elas contribuíram para o aumento da taxa de acerto do sistema.

\subsection{Conjunto de testes}

O conjunto de testes utilizado para a avaliação do sistema foi criado a partir dos currículos Lattes de 33 pesquisadores do programa de pós-graduação em Ciência da Computação da Universidade Federal de Santa Catarina (UFSC).

Destes 33 currículos, 300 publicações foram selecionadas de forma aleatória e manualmente avaliadas: caso possuíssem um registro Qualis equivalente então a publicação juntamente com o Qualis associado eram salvos no conjunto de testes; caso não possuíssem registro Qualis equivalente, então eram marcados como tal e também adicionados ao conjunto de testes.

Nas avaliações descritas a seguir, foram dadas como \textit{query} ao sistema cada uma das publicações da coleção de testes, porém sem expor os resultados manualmente avaliados. Cada resposta retornada pelo sistema foi comparada com a respectiva resposta manualmente selecionada. Em caso das respostas serem idênticas, então o sistema avaliou corretamente a publicação; em caso de não serem idênticas, avaliou incorretamente. O caso de não haver registro Qualis equivalente à publicação foi considerada uma resposta correta quando o sistema não retornou nenhum resultado, e uma resposta incorreta caso contrário.

\subsection{Métricas utilizadas}

Uma vez definido o conjunto de teste, é preciso definir as métricas utilizadas na avaliação. Através da comparação das métricas é possível concluir se houve melhora em certos aspectos do sistema. No caso do SILQ, deseja-se melhorar a taxa de acerto, ou seja, maximizar o número de trabalhos corretamente avaliados. As métricas clássicas de avaliação de sistemas IR discutidas na Seção \ref{sec:avaliacao-ir} foram consideradas.

As métricas de precisão e revocação foram descartadas por não se encaixarem com a forma de avaliação do sistema, baseada em \textit{rank}. Conforme já discutido, estas métricas não são indicadas para sistemas deste tipo. Medidas mais indicadas nesse caso são \textit{Precision at k} (\textit{P@k}) e \textit{R-Precision}. O algoritmo de avaliação do SILQ, porém, considera apenas o primeiro registro Qualis retornado para realizar \textit{match} com o trabalho sendo avaliado (apenas o mais similar). Neste caso, a avaliação usando estas duas métricas devem considerar apenas o primeiro resultado, ou seja, \textit{P@1} e \textit{R-Precision} com $|R| = 1$ (sendo $R$ o número de registros relevantes para a \textit{query}). Em ambos os casos, para cada \textit{match} retornado pelo sistema, temos medidas com valor igual a 0, caso o sistema não tenha avaliado corretamente o trabalho, e 1 caso tenha avaliado corretamente. Têm-se, portanto, um simples valor \textit{booleano} indicando se houve acerto ou não, para cada \textit{query} submetida. Considerando todo o conjunto de testes, pode-se somar o número de acertos e dividir pelo tamanho do conjunto, resultado um valor que indica a \textit{taxa de acerto} do sistema. Este valor também é conhecido como \textit{exatidão}\footnote{O termo utilizado na literatura é \textit{accuracy}, cuja tradução usual é \textit{precisão}. Optou-se pelo uso do termo \textit{exatidão}, no entanto, para evitar confusões com a métrica de precisão.} e foi a medida base escolhida para a  avaliação experimental do sistema.

Outra medida utilizada em um primeiro momento foi a Média de Rank Recíproco (MRR). Conforme discutido, ela é particularmente interessante para sistemas que produzem uma lista de resultados ordenados por probabilidade de corretude, e, ao contrário da exatidão, é capaz de modelar o quão bem o sistema classificou o resultado correto, mesmo quando ele não foi classificado em primeiro lugar.

Por estas razões, as medidas de exatidão (ou taxa de acerto) e MRR foram escolhidas para as avaliações experimentais descritas nas próximas subseções.

\subsection{Avaliação de \textit{threshold} ideal}

 Um dos questionamentos levantados no início deste trabalho e que geralmente ocorre ao projetar sistemas de \textit{data matching} baseados em similaridade, é o de qual \textit{threshold} utilizar. Na primeira versão do sistema foi introduzido um controle de \quotes{nível de confiança} que permitia ao usuário controlar o \textit{threshold} utilizado pelo algoritmo, conforme detalhado na Seção \ref{sec:algoritmo-silq1}. O nível de confiança padrão, porém, foi fixado em 60\% (equivalente ao \textit{threshold} de valor $0.6$). Este valor foi provavelmente escolhido de forma empírica pois observou-se que maximizava o número de resultados corretos, porém não foram realizados experimentos comprovando esta teoria.

 Desta forma, para encontrar o valor de \textit{threshold} ideal foi utilizado o conjunto de testes para avaliar o algoritmo inicial do SILQ 1, em um primeiro momento. O método utilizado foi o de avaliar via sistema cada uma das publicações do conjunto de testes e comparar com o resultado real, e repetir o processo variando o \textit{threshold} a fim de observar as médias de exatidão e de rank recíproco (MRR). Os resultados foram agrupados no gráfico da Figura \ref{fig:avaliacao-threshold}. A linha em azul claro representa a exatidão, ou seja, a taxa de trabalhos corretamente avaliados pelo sistema. A linha em cinza representa a média de rank recíproco (MRR), calculada conforme discutido na Seção \ref{sec:mrr}.

 \begin{figure}[!h]
    \centering
    \caption{Taxa de trabalhos corretamente avaliados e Média de Rank Recíproco (MRR) para diferentes \textit{thresholds}}
    \includegraphics[width=\textwidth]{figuras/avaliacao-threshold.png}
    \label{fig:avaliacao-threshold}
 \end{figure}

O primeiro fenômeno que observamos ao avaliar o gráfico é o ponto de máximo por volta do valor $0.55$ de \textit{threshold}, que totaliza uma exatidão aproximada de 88\%, e a tendência da exatidão baixar ao se afastar deste pico, para ambas as direções. Esse é um comportamento esperado pois valores de \textit{threshold} baixos tendem a diminuir a exatidão do sistema por retornar resultados não relevantes para as \textit{queries}, enquanto valores altos tendem a diminuir a exatidão por deixar de retornar resultados relevantes. Este ponto máximo trata-se, portanto, do \textit{threshold} ideal para o caso de testes em questão.

Outra característica observada é a tendência do valor de MRR acompanhar o da exatidão, sendo sempre igual ou apenas um pouco superior em magnitude. Isso acontece pela forma com que o MRR é calculado, atribuindo valor de $1/r$ a cada avaliação, sendo $r$ a posição em que o resultado real foi avaliado pelo sistema. Se o resultado foi corretamente avaliado, portanto, o valor de $1/1 = 1$ é atribuído ao resultado, o mesmo valor que seria atribuído à exatidão, já que o conjunto de valores possíveis para esta métrica é $\{0, 1\}$ para cada resultado (0 representando um erro e 1 representando um acerto). A semelhança dos valores, portanto, indica que houveram poucos casos em que o algoritmo retornou o resultado real em posições inferiores à primeira no \textit{rank} de avaliação. Esta característica do valor de MRR permaneceu constante nos demais testes realizados neste trabalho, portanto omitiu-se o valor de MRR nas demais avaliações.

\subsection{Avaliação dos algoritmos} \label{sec:avaliacao-algoritmos}

Os algoritmos descritos na Seção \ref{sec:algoritmo} foram avaliados utilizando o mesmo processo descrito na Seção anterior. O algoritmo \textit{trigrams} trata-se do método inicial utilizado pelo SILQ 1 e cuja análise de \textit{threshold} ideal foi realizada na Seção anterior. O algoritmo \texttt{fb(t)} foi testado variando $t$ nos valores que obtiveram melhores resultados. Todas as análise foram realizadas com valor de \textit{treshold} igual a $0.55$. A Tabela \ref{tab:comparacao-algoritmos} apresenta os resultados de cada teste.

\begin{table}[!h]
\begin{center}
\caption{Comparação da exatidão dos diferentes algoritmos testados (utilizando \textit{threshold} de $0.55$)}
\label{tab:comparacao-algoritmos}
\begin{tabular}{ c | c }
\hline
\textbf{Algoritmo} & \textbf{Exatidão} \\
\hline

\textit{trigrams} & 88.667\% \\
\textit{fb(1.00)} & 89.667\% \\
\textit{fb(0.90)} & 90.667\% \\
\textit{fb(0.80)} & 92.667\% \\
\textit{fb(0.70)} & 92.667\% \\
\textit{fb(0.60)} & 91.000\% \\
\textit{query\_aliasing} & \textbf{93.333}\% \\

\end{tabular}
\end{center}
\end{table}

A primeira tentativa de usar \textit{feedback} de relevância na avaliação foi com o algoritmo \textit{fb(1.00)}, que considera os resultados informados pelo usuário quando a \textit{query} é idêntica a algum \textit{feedback}. Houve uma melhora na taxa de acertos, porém de forma não tão significativa.

As variações que utilizam valores menores de $t$, porém, obtiveram melhores resultados, por serem capazes de identificar \textit{queries} similares aos \textit{feedbacks} já informados pelo usuário, mesmo quando este não julgou exatamente a \textit{query} em questão. Um exemplo que demonstra este fato são os nomes de eventos \textit{\quotes{IEEE International Symposium on Computer-Based Medical Systems}} e \textit{\quotes{27th International Symposium on ComputerBased Medical Systems (CBMS)}}, extraídos de um mesmo currículo Lattes. É fácil notar que tratam-se do mesmo evento, porém o usuário informou a sigla e o número da edição no segundo, e nenhuma destas informações no primeiro (além de não utilizar o hífen em um dos casos). Caso o usuário tenha dado \textit{feedback} para somente um dos casos, os algoritmos \texttt{fb(t)} com valores de $t$ inferiores a $1.0$ são capazes de utilizar o mesmo \textit{feedback} para ambos, apesar das \textit{queries} não serem idênticas.

Valores de $t$ muito baixos, porém, deterioram rapidamente a taxa de acerto pois consideram \textit{feedbacks} similares entre eventos que não tem relação. Desta forma, existe também um \quotes{valor ideal} de $t$, que gira em torno de $0.7$ a $0.8$ conforme os testes realizados.

O algoritmo \texttt{fb(t)}, porém, pode cometer \quotes{injustiças} pois considera os \textit{feedbacks} como resultados corretos, independente dos resultados primários retornados, caso sejam superiores ao \textit{threshold} $t$. O algoritmo de \texttt{query\_aliasing} resolve este problema inserindo o \textit{feedback} no \textit{ranking} junto com os demais resultados. O melhor resultado (aquele mais similar à \textit{query}) será utilizado, independente da técnica usada para obtê-lo. Nos testes realizados, o algoritmo de \texttt{query\_aliasing} obteve a melhor taxa de acerto, com uma média de 93.3\% de trabalhos corretamente avaliados.

Foi realizada uma última avaliação comparativa entre o algoritmo \textit{trgm} inicial e o novo que utiliza \texttt{query\_aliasing}. A Figura \ref{fig:avaliacao-algoritmos} mostra a taxa de acerto média para ambos os algoritmos variando o \textit{threshold} utilizado. Nota-se que o algoritmo que utiliza \textit{feedback} de relevância obteve melhores resultados, para qualquer \textit{threshold} utilizado, aumentando em aproximadamente 6\% a taxa de acerto. O \textit{threshold} ideal, porém, se manteve constante em $0.55$ pois é dependente da função de similaridade.

\begin{figure}[!h]
   \centering
   \caption{Comparação da taxa de acerto do algoritmo \textit{trgm} e do \textit{trgm + query\_aliasing} para diferentes \textit{thresholds}}
   \includegraphics[width=\textwidth]{figuras/avaliacao-algoritmos.png}
   \label{fig:avaliacao-algoritmos}
\end{figure}
