% !TEX root = main.tex

\chapter{Avaliação do uso de Feedback de relevância} \label{cap:avaliacao}

Um dos itens de melhoria indicados por \cite{Silq1} após a realização da primeira versão do sistema seria ``permitir que o usuário auxilie a ferramenta na qualificação", conforme relatado na seção \ref{sec:trabalhos-futuros-silq1}. Em sistemas de IR, esta característica é denominada \textit{Feedback de Relevância}.

A ideia por trás desta técnica é permitir que o usuário julgue resultados iniciais retornados pelo sistema, classificando-os como relevantes ou não para a \textit{query} vigente. O sistema então é capaz de utilizar esta informação para melhorar seu algoritmo de classificação e retornar melhores resultados para novas \textit{queries}.

O capítulo atual apresenta como esta técnica foi aplicada no SILQ, qual modelo lógico foi utilizado para o armazenamento dos \textit{feedbacks} e qual técnica utilizada para sua obtenção. Por fim, são apresentadas medições realizadas no sistema que validam se a técnica foi relevante para o aumento de performance.

\section{Obtenção do feedback} \label{sec:feedback}

Em sistemas de IR que implementam \textit{feedback} de relevância, tipicamente é permitido ao usuário julgar como relevante ou não relevante cada item retornado pelo sistema. Isso é natural pois o objetivo deste tipo de sistema é retornar a totalidade do conjunto de itens relevantes, cujo tamanho é variável dependendo da \textit{query}. O SILQ, porém, é um caso específico em que existem somente 0 ou 1 item relevante para toda \textit{query}. Isso acontece pois cada trabalho indicado no currículo Lattes de um pesquisador é marcado como publicado ou apresentado em somente um evento. O objetivo do algoritmo de avaliação do SILQ é deduzir que evento é este, caso exista na base de dados Qualis.

Para a implementação de \textit{feedback} de relevância no SILQ, portanto, não é necessário o julgamento de cada item retornado pelo sistema em uma avaliação, mas apenas marcar \textit{qual} dos itens da base de dados Qualis é um \textit{match} correto para a \textit{query} vigente. Desta forma, o usuário deve ser capaz de indicar o registro Qualis que deve ser considerado para cada trabalho avaliado. Também existe o caso especial em que não existe um registro Qualis correspondente ao trabalho, neste caso o usuário deve ser capaz de indicar que não existem \textit{matches} corretos para o trabalho.

A Figura \ref{fig:feedbacks} mostra o exemplo de três trabalhos conceituados, em uma avaliação de currículo Lattes, e que receberam \textit{feedback} do usuário. O botão de ``joinha" é utilizado para marcar o resultado que o usuário considera relevante para cada trabalho (os botões grifados marcam aqueles que já foram marcados). Neste caso, o registro Qualis marcado como relevante é associado à \textit{query} atual, que engloba o título, ano e área do trabalho avaliado, juntamente com o usuário que está realizando o julgamento. Também é possível marcar a opção ``Nenhum registro Qualis correspondente" ou sugerir algum resultado não retornado previamente pelo sistema em ``Sugerir matching".

\begin{figure}[!h]
   \centering
   \caption{Detalhe dos controles de feedback de relevância na página de avaliação de currículo Lattes}
   \includegraphics[width=\textwidth]{figuras/feedbacks.png}
   \label{fig:feedbacks}
\end{figure}

Os \textit{feedbacks} de relevância dados pelos usuários são salvos na base de dados SILQ na tabela \textit{tb\_feedback} (Figura \ref{fig:modelo-logico}) para uso posterior pelo algoritmo de classificação, conforme descrito na seção \ref{sec:algoritmo}. Já que um trabalho qualquer pode ter no máximo um registro Qualis associado, são salvas somente um \textit{feedback} por \textit{query} por usuário. Desta forma, uma chave única é utilizada na tabela de \textit{feedbacks}, dada pela dupla (título do trabalho, usuário).

\subsection{Algoritmo de avaliação com feedback} \label{sec:algoritmo}


Uma vez registrados os \textit{feedbacks} dos usuários, passa-se a questionar de que forma utilizá-los para aumentar a performance do sistema. A seção atual apresenta dois algoritmos que foram testados neste trabalho e como foram implementados, enquanto a \afazer{próxima seção} apresenta os resultados e comparações de precisão dos mesmos.

\textbf{Algoritmo \textit{fb(t)}:}

Umas das abordagens mais simples que podem ser usadas neste caso é utilizar o resultado marcado pelo usuário sempre que uma \textit{query} idêntica ao do \textit{feedback} seja submetida ao sistema. O algoritmo \textit{fb(1.0)}, portanto, foi desenvolvido com base nesta ideia. O valor ``$1.0$" presente no nome do algoritmo apenas indica que o \textit{feedback} é considerado em detrimento de qualquer outro resultado dado pelo sistema quando a \textit{query} submetida for 100\% similar (ou seja, idêntica) à \textit{query} do \textit{feedback}.

\begin{lstlisting}[language=java, caption=Algoritmo \textit{fb(1.0)}]
public List<Conceito> avaliar(String query, Usuario usuarioLogado) {
   List<Conceito> conceitos = getConceitosTrigrams(query);

   Feedback feedback = getConceitoFeedback(query, usuarioLogado);
   Conceito feedbackConceito = feedbackToConceito(feedback);
   conceitos.add(feedbackConceito);

   return conceitos;
}
\end{lstlisting}

Uma modificação que pode ser feita no mesmo algoritmo é utilizar \textit{thresholds de feedback} mais baixos. O \textit{fb(1.0)} considera apenas \textit{feedbacks} que sejam idênticos à \textit{query} submetida, porém pode ser que sejam obtidos resultados melhores ao considerar \textit{feedbacks} textualmente similares à \textit{query} atual, considerando certo \textit{threhold}. O algoritmo \textit{fb(t)} (para $0.0 \le t \le 1.0$) é uma generalização de \textit{fb(1.0)} que considera \textit{feedbacks} cuja similaridade textual em relação à \textit{query} seja maior que o \textit{treshold} $t$. Por exemplo, \textit{fb(0.75)} irá considerar \textit{feedbacks} cujo valor de similaridade textual em relação à \textit{query} seja $0.75$ ou superior. Neste caso o \textit{feedback} é retornado em detrimento dos demais resultado encontrados pelo sistema.

O algoritmo \textit{fb(t)}, entretanto, leva a outros questionamentos, já que utiliza a mesma técnica de \textit{data matching} que foi proposta a melhorar. Qual o valor de $t$ (\textit{trehshold de feedback}) ideal? Qual o algoritmo de similaridade textual ideal para este caso? A seção \ref{sec:avaliacao-algoritmos} apresenta testes de validação do algoritmo \textit{fb(t)} para diferentes valores de $t$.

\textbf{Algoritmo \textit{query\_aliasing}:}

Outra estratégia que mostrou-se de mais fácil implementação e melhor desempenho foi considerar os \textit{feedbacks} do usuário e inserí-los no \textit{rank} de resultados do sistema com base em seu valor de similaridade textual em relação à \textit{query} submetida.

Ou seja, ao processar uma \textit{query} $q$ qualquer, o sistema processa um \textit{rank} de resultados primários com base no algoritmo \textit{trigrams} inicial. O \textit{rank} é ordenado do resultado mais similar à $q$ ao menos similar. Após esta etapa, ele também compara $q$ com cada uma das \textit{queries} atreladas aos \textit{feedbacks} de relevância dados pelo usuário utilizando o mesmo algoritmo de similaridade textual (\textit{trigrams} neste caso). O resultado mais similar, digamos $f$, é inserido no \textit{ranking} de resultados.

Desta forma, se $q$ é idêntico a um \textit{feedback} já submetido pelo usuário, $f$ será retornado e inserido no topo do \textit{ranking} de resultados, por ser 100\% similar à \textit{query}. Outros resultados similares, porém não idênticos, serão inseridos no \textit{ranking} conforme seu valor de similaridade e só serão escolhidos em detrimento de outros resultados primários se seus valores de similaridade forem superiores a eles.

É como se, ao dar um \textit{feedback} de relevância qualquer, o usuário criasse um \textit{alias} (um apelido) ao resultado que está sugerindo. Assim, o sistema deve avaliar novas \textit{queries} não só comparando-as com o nome real dos documentos, mas também com os apelidos dados a eles pelo usuário. Por este motivo o algoritmo foi chamado de \textit{query\_aliasing}. A avaliação deste algoritmo foi realizada e comparada com os demais na seção \ref{sec:avaliacao-algoritmos}.

\section{Avaliação} \label{sec:validation}

Esta \afazer{seção} apresenta os procedimentos realizados para avaliar as alterações promovidas no algoritmo de avaliação da nova versão do SILQ e se elas contribuíram para o aumento da precisão do sistema.

\subsection{Conjunto de testes}

O conjunto de testes utilizado para a avaliação do sistema foi criado a partir dos currículos Lattes de 33 pesquisadores do programa de pós-graduação em Ciência da Computação da Universidade Federal de Santa Catarina (UFSC).

Destes 33 currículos, 300 publicações foram selecionadas de forma aleatória e manualmente avaliadas: caso possuíssem um registro Qualis equivalente então a publicação juntamente com o Qualis associado eram salvos no conjunto de testes; caso não possuíssem registro Qualis equivalente, então eram marcados como tal e também adicionados ao conjunto de testes.

Nas avaliações descritas a seguir, foram dadas como \textit{query} ao sistema cada uma das publicações da coleção de testes, porém sem expor os resultados manualmente avaliados. Cada resposta retornada pelo sistema foi comparada com a respectiva resposta manualmente selecionada. Em caso das respostas serem idênticas, então o sistema avaliou corretamente a publicação; em caso de não serem idênticas, avaliou incorretamente. O caso de não haver registro Qualis equivalente à publicação foi considerada uma resposta correta quando o sistema não retornou nenhum resultado, e uma resposta incorreta caso contrário.

\subsection{Avaliação de \textit{threshold} ideal}

 Um dos questionamentos levantados no início deste trabalho e que geralmente ocorre ao projetar sistemas de \textit{data matching} baseados em similaridade, é o de qual \textit{threshold} utilizar. Na primeira versão do sistema foi introduzido um controle de ``nível de confiança" que permitia ao usuário controlar o \textit{threshold} utilizado pelo algoritmo, conforme detalhado na seção \ref{sec:algoritmo-silq1}. O nível de confiança padrão, porém, foi fixado em 60\% (equivalente ao \textit{threshold} de valor $0.6$). Este valor foi provavelmente escolhido de forma empírica pois observou-se que maximizava o número de resultados corretos, porém não foram realizados experimentos comprovando esta teoria.

 Desta forma, para encontrar o valor de \textit{threshold} ideal utilizamos nosso conjunto de testes para avaliar o algoritmo inicial do SILQ 1, em um primeiro momento. O método utilizado foi o de avaliar via sistema cada uma das publicações do conjunto de testes e comparar com o resultado real, e repetir o processo variando o \textit{threshold} a fim de observar as médias de exatidão e de rank recíproco (MRR). Os resultados foram agrupados no gráfico da Figura \ref{fig:avaliacao-threshold}. A linha em azul claro representa a exatidão, ou seja, a taxa de trabalhos corretamente avaliados pelo sistema. A linha em cinza representa a média de rank recíproco (MRR), calculada conforme discutido na seção \ref{sec:mrr}.

 \begin{figure}[!h]
    \centering
    \caption{Taxa de trabalhos corretamente avaliados e Média de Rank Recíproco (MRR) para diferentes \textit{thresholds}}
    \includegraphics[width=\textwidth]{figuras/avaliacao-threshold.png}
    \label{fig:avaliacao-threshold}
 \end{figure}

O primeiro fenômeno que observamos ao avaliar o gráfico é o ponto de máximo por volta do valor $0.55$ de \textit{threshold}, que totaliza uma exatidão aproximada de 88\%, e a tendência da exatidão baixar ao se afastar deste pico, para ambas as direções. Esse é um comportamento esperado pois valores de \textit{threhold} baixos tendem a diminuir a precisão do sistema por retornar resultados não relevantes para as \textit{queries}, enquanto valores altos tendem a dimuir a precisão por deixar de retornar resultados relevantes. Este ponto máximo trata-se, portanto, do \textit{threshold} ideal para o caso de testes em questão.

Outra característica observada é a tendência do valor de MRR acompanhar o da exatidão, sendo sempre igual ou apenas um pouco superior em magnetude. Isso acontece pela forma com que o MRR é calculado, atribuindo valor de $1/r$ a cada avaliação, sendo $r$ a posição em que o resultado real foi avaliado pelo sistema. Se o resultado foi corretamente avaliado, portanto, o valor de $1/1 = 1$ é atribuído ao resultado, o mesmo valor que seria atribuído à exatidão (cuja imagem é somente $\{0, 1\}$ para cada resultado). A semelhança dos valores, portanto, indica que houveram poucos casos em que o algoritmo retornou o resultado real em posições inferiores à primeira no \textit{rank} de avaliação. Esta característica do valor de MRR permaneceu constante nos demais testes realizados neste trabalho, portanto omitiu-se o valor de MRR nas demais avaliações.

\subsection{Avaliação dos algoritmos} \label{sec:avaliacao-algoritmos}

\begin{table}[!h]
\begin{center}
\caption{Comparação da exatidão dos diferentes algoritmos testados (utilizando \textit{threshold} de $0.55$)}
\label{tab:lala}
\begin{tabular}{ c | c }
\hline
\textbf{Algoritmo} & \textbf{Exatidão} \\
\hline

\textit{trgm} & 88.667\% \\
\textit{trgm + fb(1.00)} & 89.667\% \\
\textit{trgm + fb(0.90)} & 90.667\% \\
\textit{trgm + fb(0.80)} & 92.667\% \\
\textit{trgm + fb(0.70)} & 92.667\% \\
\textit{trgm + fb(0.60)} & 91.000\% \\
\textit{trgm + query\_aliasing} & \textbf{93.333}\% \\

\end{tabular}
\end{center}
\end{table}

\begin{figure}[!h]
   \centering
   \caption{Comparação do algoritmo inicial do SILQ 1 (\textit{trgm}) e do novo \textit{trgm + query\_aliasing} baseado em \textit{feedback} de relevância}
   \includegraphics[width=\textwidth]{figuras/avaliacao-algoritmos.png}
   \label{fig:avaliacao-algoritmos}
\end{figure}

Mensurações realizadas.

Threshold ideal.
